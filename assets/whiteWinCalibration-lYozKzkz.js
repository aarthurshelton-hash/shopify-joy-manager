const n="/**\n * White Win Calibration System v7.75\n * \n * Addresses systematic under-prediction of white wins discovered in benchmark analysis:\n * - En Pensent: 44.4% accuracy on white wins vs 58.0% on black wins\n * - Predicts black_wins 31% more often than white_wins\n * \n * This module provides calibration factors to correct for the bias.\n */\n\nimport { StrategicArchetype } from '../colorFlowAnalysis';\n\n/**\n * Historical bias analysis from 9,500+ games:\n * - White actually wins: 48% of games\n * - Black actually wins: 45% of games  \n * - Draws: 7% of games\n * \n * But En Pensent predicts:\n * - black_wins: 56% of predictions\n * - white_wins: 42% of predictions\n * - draw: 2% of predictions\n * \n * Correction: Apply white bias boost when patterns are ambiguous\n */\n\nexport const WHITE_WIN_PRIOR = 0.48;  // Actual white win rate in dataset\nexport const BLACK_WIN_PRIOR = 0.45;  // Actual black win rate in dataset\nexport const DRAW_PRIOR = 0.07;       // Actual draw rate in dataset\n\n/**\n * Bias correction factors per archetype\n * Positive values boost white win predictions\n * Based on analysis of where En Pensent most under-predicts white\n */\nexport const ARCHETYPE_WHITE_BIAS_CORRECTION: Record<StrategicArchetype, number> = {\n  // v7.85: CRITICAL FIX - prophylactic_defense was causing 70% of white win losses\n  // When we classify as prophylactic_defense, we're often wrong about black having initiative\n  prophylactic_defense: 0.25,    // HIGHEST boost - this archetype massively under-predicts white\n  \n  // Most problematic - severely under-predicts white\n  central_domination: 0.20,      // White dominates center → often wins\n  piece_harmony: 0.18,           // Coordination often favors white's initiative\n  kingside_attack: 0.15,         // White's first-move kingside attacks succeed\n  positional_squeeze: 0.15,      // White's space advantage converts\n  \n  // Moderate under-prediction\n  queenside_expansion: 0.12,     // White queenside play effective\n  pawn_storm: 0.10,              // White pawn storms with tempo\n  sacrificial_attack: 0.08,      // White sacs with compensation\n  closed_maneuvering: 0.10,      // v7.85: was 0.05, boost for quiet white wins\n  \n  // Balanced prediction\n  endgame_technique: 0.05,       // Relatively accurate\n  \n  // Over-predicts white (negative correction)\n  open_tactical: -0.05,          // Actually balanced in tactics\n  opposite_castling: -0.08,      // Mutual attacks - draws more common\n  \n  // Unknown - use neutral\n  unknown: 0.0,\n};\n\n/**\n * Get calibrated outcome prediction accounting for white win bias\n */\nexport function calibrateForWhiteBias(\n  rawPrediction: 'white_wins' | 'black_wins' | 'draw' | 'unclear',\n  archetype: StrategicArchetype,\n  dominantSide: 'white' | 'black' | 'contested',\n  stockfishEval: number,\n  confidence: number\n): {\n  calibratedPrediction: 'white_wins' | 'black_wins' | 'draw' | 'unclear';\n  adjustmentApplied: string;\n  adjustmentMagnitude: number;\n} {\n  const biasCorrection = ARCHETYPE_WHITE_BIAS_CORRECTION[archetype] || 0;\n  \n  // Don't recalibrate high-confidence predictions\n  if (confidence > 70) {\n    return {\n      calibratedPrediction: rawPrediction,\n      adjustmentApplied: 'High confidence - no calibration',\n      adjustmentMagnitude: 0,\n    };\n  }\n  \n  // Don't recalibrate when Stockfish strongly disagrees with our prediction\n  const stockfishFavorsWhite = stockfishEval > 50;\n  const stockfishFavorsBlack = stockfishEval < -50;\n  \n  // v7.88 EQUILIBRIUM: Minimal intervention - trust the engine more\n  // Data shows: We beat SF on black wins (48.5% vs 33.4%), SF beats us on white (70.8% vs 53.6%)\n  // The solution is NOT to flip aggressively, but to improve underlying pattern recognition\n  \n  // ONLY flip in extreme disagreement cases with high Stockfish confidence\n  if (rawPrediction === 'black_wins') {\n    // Flip ONLY when Stockfish is VERY confident white is winning (eval > 150)\n    // AND our confidence is low (< 55)\n    const extremeWhiteAdvantage = stockfishEval > 150 && confidence < 55;\n    \n    if (extremeWhiteAdvantage) {\n      return {\n        calibratedPrediction: 'white_wins',\n        adjustmentApplied: `v7.88 Equilibrium: SF strongly favors white (${stockfishEval}cp)`,\n        adjustmentMagnitude: biasCorrection,\n      };\n    }\n  }\n  \n  // Mirror logic for white_wins predictions when SF strongly disagrees\n  if (rawPrediction === 'white_wins') {\n    const extremeBlackAdvantage = stockfishEval < -150 && confidence < 55;\n    \n    if (extremeBlackAdvantage) {\n      return {\n        calibratedPrediction: 'black_wins',\n        adjustmentApplied: `v7.88 Equilibrium: SF strongly favors black (${stockfishEval}cp)`,\n        adjustmentMagnitude: -biasCorrection,\n      };\n    }\n  }\n  \n  // When prediction is unclear, use priors\n  if (rawPrediction === 'unclear') {\n    // In ambiguous cases, slight lean toward white given prior\n    if (stockfishEval > 20 && biasCorrection > 0) {\n      return {\n        calibratedPrediction: 'white_wins',\n        adjustmentApplied: 'Prior-based: unclear → white (dataset prior)',\n        adjustmentMagnitude: WHITE_WIN_PRIOR - 0.33,\n      };\n    }\n    if (stockfishEval < -20 && biasCorrection < 0) {\n      return {\n        calibratedPrediction: 'black_wins',\n        adjustmentApplied: 'Prior-based: unclear → black',\n        adjustmentMagnitude: BLACK_WIN_PRIOR - 0.33,\n      };\n    }\n  }\n  \n  // When contested and predicting white but archetype over-predicts white\n  if (rawPrediction === 'white_wins' && biasCorrection < -0.05 && !stockfishFavorsWhite) {\n    // Don't flip, but note the potential issue\n    return {\n      calibratedPrediction: rawPrediction,\n      adjustmentApplied: `Warning: ${archetype} may over-predict white`,\n      adjustmentMagnitude: biasCorrection,\n    };\n  }\n  \n  return {\n    calibratedPrediction: rawPrediction,\n    adjustmentApplied: 'No calibration needed',\n    adjustmentMagnitude: 0,\n  };\n}\n\n/**\n * Get overall bias health metrics\n */\nexport function getBiasHealthMetrics(predictions: {\n  predicted: string;\n  actual: string;\n  archetype: StrategicArchetype;\n}[]): {\n  whiteWinAccuracy: number;\n  blackWinAccuracy: number;\n  drawAccuracy: number;\n  biasRatio: number;  // < 1 means under-predicting white\n  worstArchetypes: { archetype: string; accuracy: number }[];\n} {\n  let whiteCorrect = 0, whiteTotal = 0;\n  let blackCorrect = 0, blackTotal = 0;\n  let drawCorrect = 0, drawTotal = 0;\n  let whitePredictions = 0, blackPredictions = 0;\n  \n  const archetypeStats: Record<string, { correct: number; total: number }> = {};\n  \n  for (const p of predictions) {\n    if (p.predicted === 'white_wins') whitePredictions++;\n    if (p.predicted === 'black_wins') blackPredictions++;\n    \n    if (p.actual === 'white_wins') {\n      whiteTotal++;\n      if (p.predicted === 'white_wins') whiteCorrect++;\n    } else if (p.actual === 'black_wins') {\n      blackTotal++;\n      if (p.predicted === 'black_wins') blackCorrect++;\n    } else {\n      drawTotal++;\n      if (p.predicted === 'draw') drawCorrect++;\n    }\n    \n    if (!archetypeStats[p.archetype]) {\n      archetypeStats[p.archetype] = { correct: 0, total: 0 };\n    }\n    archetypeStats[p.archetype].total++;\n    if (p.predicted === p.actual) {\n      archetypeStats[p.archetype].correct++;\n    }\n  }\n  \n  const worstArchetypes = Object.entries(archetypeStats)\n    .map(([archetype, stats]) => ({\n      archetype,\n      accuracy: stats.total > 0 ? (stats.correct / stats.total) * 100 : 0,\n    }))\n    .sort((a, b) => a.accuracy - b.accuracy)\n    .slice(0, 5);\n  \n  return {\n    whiteWinAccuracy: whiteTotal > 0 ? (whiteCorrect / whiteTotal) * 100 : 0,\n    blackWinAccuracy: blackTotal > 0 ? (blackCorrect / blackTotal) * 100 : 0,\n    drawAccuracy: drawTotal > 0 ? (drawCorrect / drawTotal) * 100 : 0,\n    biasRatio: blackPredictions > 0 ? whitePredictions / blackPredictions : 1,\n    worstArchetypes,\n  };\n}\n";export{n as default};
