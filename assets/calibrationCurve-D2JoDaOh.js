const e="/**\n * Prediction Calibration Curve Module\n * Tracks predicted confidence vs actual accuracy\n * \"When we say 70% confident, are we right 70% of the time?\"\n */\n\nexport interface CalibrationBucket {\n  rangeStart: number;  // e.g., 0.60\n  rangeEnd: number;    // e.g., 0.70\n  predictions: number; // Count of predictions in this range\n  correct: number;     // Count of correct predictions\n  expectedAccuracy: number; // Mid-point of range (what we claimed)\n  actualAccuracy: number;   // correct / predictions\n  calibrationError: number; // |expected - actual|\n}\n\nexport interface CalibrationRecord {\n  id: string;\n  timestamp: number;\n  predictedConfidence: number;\n  predictedDirection: 'up' | 'down' | 'neutral';\n  actualDirection?: 'up' | 'down' | 'neutral';\n  wasCorrect?: boolean;\n  resolvedAt?: number;\n  symbol?: string;\n  timeHorizon?: number;\n}\n\nexport interface CalibrationMetrics {\n  totalPredictions: number;\n  resolvedPredictions: number;\n  overallAccuracy: number;\n  expectedCalibrationError: number; // Average |expected - actual| across buckets\n  brierScore: number;               // Mean squared probability error\n  reliability: number;              // How well calibrated (0 = perfectly calibrated)\n  resolution: number;               // How much predictions vary from base rate\n  calibrationCurve: CalibrationBucket[];\n  overconfidenceBias: number;       // Positive = overconfident, Negative = underconfident\n}\n\n// Calibration Curve Tracker Class\nexport class CalibrationCurveTracker {\n  private records: CalibrationRecord[] = [];\n  private bucketWidth: number = 0.10; // 10% buckets\n\n  constructor(bucketWidth: number = 0.10) {\n    this.bucketWidth = bucketWidth;\n  }\n\n  // Record a new prediction\n  recordPrediction(\n    predictedConfidence: number,\n    predictedDirection: 'up' | 'down' | 'neutral',\n    symbol?: string,\n    timeHorizon?: number\n  ): string {\n    const id = `cal_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    this.records.push({\n      id,\n      timestamp: Date.now(),\n      predictedConfidence: Math.max(0, Math.min(1, predictedConfidence)),\n      predictedDirection,\n      symbol,\n      timeHorizon\n    });\n\n    this.pruneOldRecords();\n    return id;\n  }\n\n  // Resolve a prediction with actual outcome\n  resolvePrediction(\n    id: string,\n    actualDirection: 'up' | 'down' | 'neutral'\n  ): boolean {\n    const record = this.records.find(r => r.id === id);\n    if (!record) return false;\n\n    record.actualDirection = actualDirection;\n    record.wasCorrect = record.predictedDirection === actualDirection;\n    record.resolvedAt = Date.now();\n    \n    return true;\n  }\n\n  // Get calibration metrics\n  getCalibrationMetrics(): CalibrationMetrics {\n    const resolved = this.records.filter(r => r.wasCorrect !== undefined);\n    \n    if (resolved.length === 0) {\n      return this.getEmptyMetrics();\n    }\n\n    // Build calibration buckets\n    const buckets = this.buildCalibrationBuckets(resolved);\n    \n    // Calculate overall accuracy\n    const correct = resolved.filter(r => r.wasCorrect).length;\n    const overallAccuracy = correct / resolved.length;\n    \n    // Calculate Expected Calibration Error (ECE)\n    const expectedCalibrationError = this.calculateECE(buckets, resolved.length);\n    \n    // Calculate Brier Score\n    const brierScore = this.calculateBrierScore(resolved);\n    \n    // Calculate reliability and resolution\n    const { reliability, resolution } = this.calculateReliabilityResolution(buckets, overallAccuracy, resolved.length);\n    \n    // Calculate overconfidence bias\n    const overconfidenceBias = this.calculateOverconfidenceBias(buckets);\n\n    return {\n      totalPredictions: this.records.length,\n      resolvedPredictions: resolved.length,\n      overallAccuracy,\n      expectedCalibrationError,\n      brierScore,\n      reliability,\n      resolution,\n      calibrationCurve: buckets,\n      overconfidenceBias\n    };\n  }\n\n  // Build calibration buckets\n  private buildCalibrationBuckets(resolved: CalibrationRecord[]): CalibrationBucket[] {\n    const buckets: CalibrationBucket[] = [];\n    \n    for (let start = 0; start < 1; start += this.bucketWidth) {\n      const end = start + this.bucketWidth;\n      const inBucket = resolved.filter(r => \n        r.predictedConfidence >= start && r.predictedConfidence < end\n      );\n      \n      const correctInBucket = inBucket.filter(r => r.wasCorrect).length;\n      const expectedAccuracy = (start + end) / 2;\n      const actualAccuracy = inBucket.length > 0 ? correctInBucket / inBucket.length : 0;\n      \n      buckets.push({\n        rangeStart: start,\n        rangeEnd: end,\n        predictions: inBucket.length,\n        correct: correctInBucket,\n        expectedAccuracy,\n        actualAccuracy,\n        calibrationError: Math.abs(expectedAccuracy - actualAccuracy)\n      });\n    }\n    \n    return buckets;\n  }\n\n  // Calculate Expected Calibration Error\n  private calculateECE(buckets: CalibrationBucket[], totalPredictions: number): number {\n    let ece = 0;\n    \n    for (const bucket of buckets) {\n      if (bucket.predictions > 0) {\n        const weight = bucket.predictions / totalPredictions;\n        ece += weight * bucket.calibrationError;\n      }\n    }\n    \n    return ece;\n  }\n\n  // Calculate Brier Score (lower is better)\n  private calculateBrierScore(resolved: CalibrationRecord[]): number {\n    let sum = 0;\n    \n    for (const record of resolved) {\n      const outcome = record.wasCorrect ? 1 : 0;\n      const confidence = record.predictedConfidence;\n      sum += Math.pow(confidence - outcome, 2);\n    }\n    \n    return sum / resolved.length;\n  }\n\n  // Calculate reliability and resolution components\n  private calculateReliabilityResolution(\n    buckets: CalibrationBucket[], \n    overallAccuracy: number,\n    totalPredictions: number\n  ): { reliability: number; resolution: number } {\n    let reliability = 0;\n    let resolution = 0;\n    \n    for (const bucket of buckets) {\n      if (bucket.predictions > 0) {\n        const weight = bucket.predictions / totalPredictions;\n        \n        // Reliability: how close actual is to predicted confidence\n        reliability += weight * Math.pow(bucket.actualAccuracy - bucket.expectedAccuracy, 2);\n        \n        // Resolution: how much actual varies from base rate\n        resolution += weight * Math.pow(bucket.actualAccuracy - overallAccuracy, 2);\n      }\n    }\n    \n    return { reliability, resolution };\n  }\n\n  // Calculate overconfidence bias\n  private calculateOverconfidenceBias(buckets: CalibrationBucket[]): number {\n    let totalBias = 0;\n    let totalWeight = 0;\n    \n    for (const bucket of buckets) {\n      if (bucket.predictions > 0) {\n        // Positive bias = overconfident (expected > actual)\n        const bias = bucket.expectedAccuracy - bucket.actualAccuracy;\n        totalBias += bias * bucket.predictions;\n        totalWeight += bucket.predictions;\n      }\n    }\n    \n    return totalWeight > 0 ? totalBias / totalWeight : 0;\n  }\n\n  // Get empty metrics for when no data available\n  private getEmptyMetrics(): CalibrationMetrics {\n    return {\n      totalPredictions: this.records.length,\n      resolvedPredictions: 0,\n      overallAccuracy: 0,\n      expectedCalibrationError: 0,\n      brierScore: 0,\n      reliability: 0,\n      resolution: 0,\n      calibrationCurve: [],\n      overconfidenceBias: 0\n    };\n  }\n\n  // Get calibration advice based on metrics\n  getCalibrationAdvice(): {\n    status: 'well_calibrated' | 'overconfident' | 'underconfident' | 'insufficient_data';\n    advice: string;\n    adjustmentFactor: number;\n  } {\n    const metrics = this.getCalibrationMetrics();\n    \n    if (metrics.resolvedPredictions < 30) {\n      return {\n        status: 'insufficient_data',\n        advice: 'Need at least 30 resolved predictions for reliable calibration analysis',\n        adjustmentFactor: 1.0\n      };\n    }\n    \n    if (Math.abs(metrics.overconfidenceBias) < 0.05) {\n      return {\n        status: 'well_calibrated',\n        advice: 'Predictions are well-calibrated. Confidence levels match actual accuracy.',\n        adjustmentFactor: 1.0\n      };\n    }\n    \n    if (metrics.overconfidenceBias > 0.05) {\n      const adjustment = 1 - (metrics.overconfidenceBias * 0.5);\n      return {\n        status: 'overconfident',\n        advice: `Predictions are overconfident by ${(metrics.overconfidenceBias * 100).toFixed(1)}%. Consider reducing confidence levels.`,\n        adjustmentFactor: adjustment\n      };\n    }\n    \n    const adjustment = 1 + (Math.abs(metrics.overconfidenceBias) * 0.5);\n    return {\n      status: 'underconfident',\n      advice: `Predictions are underconfident by ${(Math.abs(metrics.overconfidenceBias) * 100).toFixed(1)}%. Can increase confidence levels.`,\n      adjustmentFactor: adjustment\n    };\n  }\n\n  // Prune records older than 90 days\n  private pruneOldRecords(): void {\n    const ninetyDaysAgo = Date.now() - (90 * 24 * 60 * 60 * 1000);\n    this.records = this.records.filter(r => r.timestamp > ninetyDaysAgo);\n  }\n\n  // Export records for persistence\n  exportRecords(): CalibrationRecord[] {\n    return [...this.records];\n  }\n\n  // Import records from persistence\n  importRecords(records: CalibrationRecord[]): void {\n    this.records = records;\n  }\n\n  // Get reliability score (0-1, higher is better calibrated)\n  getReliabilityScore(): number {\n    const metrics = this.getCalibrationMetrics();\n    // Convert reliability (error metric) to score (higher = better)\n    return Math.max(0, 1 - metrics.reliability * 5);\n  }\n}\n\n// Singleton instance\nexport const calibrationTracker = new CalibrationCurveTracker(0.10);\n\nexport default calibrationTracker;\n";export{e as default};
